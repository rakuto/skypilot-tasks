name: hf-model-downloader

resources:
  cloud: aws
  region: us-west-2
  disk_size: 200
  instance_type: g5.2xlarge # Make sure set same instance you host inference server.
  image_id: ami-04846c06498d951cb # Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.2.0 (Amazon Linux 2) 20240514

# These environment variables are supposed to be overriden by CLI flag --env-file or --env.
envs:
  # Check TensorRT-LLM image from AWS Deep Learning Container image
  # https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers
  DJL_IMAGE: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122

setup: |
  set -e
  
  echo "Pulling ${DJL_IMAGE}"
  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com
  docker pull ${DJL_IMAGE}

run: |
  set -e
  
  echo "Run Ahead-of-Time compile"
  MODEL_REPO_DIR=trt_model
  rm -rf $MODEL_REPO_DIR
  mkdir -p $MODEL_REPO_DIR 
  docker run --runtime=nvidia --gpus all --shm-size 12gb \
    -v ${MODEL_REPO_DIR}:/tmp/trtllm \
    -p 8080:8080 \
    -e OPTION_MODEL_ID=$OPTION_MODEL_ID \ 
    -e OPTION_DTYPE=$OPTION_DTYPE \
    -e OPTION_TENSOR_PARALLEL_DEGREE=$OPTION_TENSOR_PARALLEL_DEGREE \
    -e OPTION_MAX_NUM_TOKENS=$OPTION_MAX_NUM_TOKENS \
    -e OPTION_MAX_INPUT_LEN=$OPTION_MAX_INPUT_LEN \
    -e OPTION_MAX_OUTPUT_LEN=$OPTION_MAX_OUTPUT_LEN \
    -e OPTION_MAX_ROLLING_BATCH_SIZE=$OPTION_MAX_ROLLING_BATCH_SIZE \
    ${DJL_IMAGE} \
    python /opt/djl/partition/trt_llm_partition.py \
    --properties_dir $PWD \
    --trt_llm_model_repo /tmp/trtllm \
    --tensor_parallel_degree $OPTION_TENSOR_PARALLE_DEGREE
  
  echo "Uploading to S3 $S3_URL"
  aws s3 sync $MODEL_REPO_DIR $S3_URL
